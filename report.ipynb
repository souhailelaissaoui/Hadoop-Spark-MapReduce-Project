{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Problem :Single-source shortest path problem. \n",
    "\n",
    "The task is to find shortest paths from a source node to all\n",
    "other nodes in the graph. This problem is solved by the Dijkstra’s algorithm, which is sequential.\n",
    "The project has a double purpose. First get familiar with Dijkstra’s algorithm, then devise a\n",
    "MapReduce version of the algorithm. As you will realise, the process is actually iterative, so the\n",
    "identified MapReduce job must be iterated a certain number of times.\n",
    "Provide both a Python-Hadoop streaming and Spark implementation of the algorithm, and test it on\n",
    "the simple graph data provided in classes.\n",
    "Optional: perform scalability experiments as for previous projects. A single comparison on a reasonable big graph would be sufficient "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The initial non processed input file is as following, we take into account that each line is of the form « v edge-label w\n",
    "w », so in order to represent a couple « v w » only the first and third elements should be retained for the sink calculation.\n",
    "\n",
    "We assume that a graph is a set of edge pairs (v,w), indicating that an edge exists from node v to node w.\n",
    "\n",
    "The analytics problem is on directed graphs, it consists of finding the best path from a specific source node that we will name the \"Initial Node\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>B</th>\n",
       "      <th>to</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5247</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578</td>\n",
       "      <td>1672</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "      <td>6623</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>532</td>\n",
       "      <td>2373</td>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>1007</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from     B   to    D\n",
       "0     4  5247  274  274\n",
       "1   578  1672  502  502\n",
       "2    94  6623  359  359\n",
       "3   532  2373  800  800\n",
       "4    64  1007    5    5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "df = pd.read_csv('graph.txt', delim_whitespace=True, names=('from', 'B', 'to', 'D'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We preprocess this input file to be used in our MapReduce streaming.\n",
    "We will need it to be at the format:\n",
    "INPUT FORMAT: This data will be used for testing the program with linux pipe line.\n",
    "1\t[2,3]/0/GRAY/-\n",
    "2\t[1,3,4,5]/inf/WHITE/-\n",
    "3\t[1,4,2]/inf/WHITE/-\n",
    "4\t[2,3]/inf/WHITE/-\n",
    "5\t[2]/inf/WHITE/-\n",
    "\n",
    "Which stands for:\n",
    "FirstNode     [SET of paired nodes]/distanceFromBeginning/Color/BestPath\n",
    "\n",
    "The firstNode is the the node from which the edges begins\n",
    "The Set of paired nodes is a list of all the nodes where an edge from firsNode ends.\n",
    "The distance is the calculated distance from the initial node to the FirstNode. It will be initialised as infinite for all unknown pairs, and as 0 for the initial node.\n",
    "The color is used to know is the node has been visited or not by our algorithm. So it's starts off as white for all the nodes, exept for the initial node which will start as GRAY to be explored by the mapper. After exploration by the mapper, the nodes will take the color \"BLACK\".\n",
    "The BestPath will be written by the mapper and reducer which will keep all the nodes that have been browsen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>d</th>\n",
       "      <th>state</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[48, 2, 63, 136, 564, 73, 377, 141, 63, 4]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 164, 129, 19, 9, 321, 12, 536]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[274, 766, 74, 23, 762, 142, 782, 394]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[51, 199, 633, 524, 79, 394]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[79, 5, 134]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from                                          to    d  state path\n",
       "0     1  [48, 2, 63, 136, 564, 73, 377, 141, 63, 4]  0.0   GRAY    -\n",
       "1     2          [1, 164, 129, 19, 9, 321, 12, 536]  inf  WHITE    -\n",
       "2     4      [274, 766, 74, 23, 762, 142, 782, 394]  inf  WHITE    -\n",
       "3     5                [51, 199, 633, 524, 79, 394]  inf  WHITE    -\n",
       "4     8                                [79, 5, 134]  inf  WHITE    -"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing:\n",
    "df = df.drop(['B', 'D'], axis=1)\n",
    "df = df.groupby('from')['to'].apply(list)\n",
    "df2 = pd.DataFrame()\n",
    "df2['from'] = df.index\n",
    "df2['to'] = df.values\n",
    "df2['d'] = math.inf\n",
    "df2['state'] = 'WHITE'\n",
    "df2['path'] = '-'\n",
    "#Seeting the initial and final nodes : \n",
    "initial = 1\n",
    "final = 316\n",
    "df2.loc[df2['from']==initial, 'd'] = 0\n",
    "df2.loc[df2['from']==initial, 'state'] = 'GRAY'\n",
    "df2\n",
    "\n",
    "file = open(\"preprofin.txt\",\"w\") \n",
    "for index, row in df2.iterrows():\n",
    "    file.write(str(row[0]) + \"\\t\" + str(row[1]) + \"/\" + str(row[2]) + \"/\" + str(row[3]) + \"/\"+ str(row[4]) + \"\\n\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapper.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The mapper takes as input format :\n",
    "code : cat easytest.txt\n",
    "1\t[2,3]/0/GRAY/-\n",
    "2\t[1,3,4,5]/inf/WHITE/-\n",
    "3\t[1,4,2]/inf/WHITE/-\n",
    "4\t[2,3]/inf/WHITE/-\n",
    "5\t[2]/inf/WHITE/-\n",
    "\n",
    "The mappers are responsible for \"exploding\" all gray nodes - e.g. for exploding all nodes that live at our current depth in the tree. for each gray node, the mappers emit a new gray node, with distance = distance + 1. they also then emit the input gray node, but colored black. (once a node has been exploded, we're done with it.)\n",
    "\n",
    "Mappers also emit all non-gray nodes, with no change.\n",
    "\n",
    "note that when the mappers \"explode\" the gray nodes and create a new node for\n",
    "each edge, they do not know what to write for the edges of this new node - so they\n",
    "leave it blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    val = line.split(\"\\t\")\n",
    "    words = val[1].split(\"/\")\n",
    "    key = val[0]\n",
    "    words[1] = float(words[1])\n",
    "    if words[2] == 'GRAY':\n",
    "        toprint = \"/\".join([words[0], str(words[1]), \"BLACK\", words[3]])\n",
    "        print '%s\\t%s' % (key, toprint)\n",
    "        if words[0] != \"EMPTY\":\n",
    "\t\t\twords[0] = eval(words[0])\n",
    "\t\t\tfor i in words[0]:\n",
    "\t\t\t\ttoprint = \"/\".join([\"EMPTY\", str(words[1] + 1), \"GRAY\", str(words[3])+ str(key)+ \",\"])\n",
    "\t\t\t\tprint '%s\\t%s' % (i, toprint)\n",
    "    else:\n",
    "        toprint = \"/\".join([words[0], str(words[1]), words[2], words[3]])\n",
    "        print '%s\\t%s' % (key, toprint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The mapper output is:\n",
    "code : cat easytest.txt |python mapper.py\n",
    "\n",
    "1       [2,3]/0.0/BLACK/-\n",
    "2       EMPTY/1.0/GRAY/-1,\n",
    "3       EMPTY/1.0/GRAY/-1,\n",
    "2       [1,3,4,5]/inf/WHITE/-\n",
    "3       [1,4,2]/inf/WHITE/-\n",
    "4       [2,3]/inf/WHITE/-\n",
    "5       [2]/inf/WHITE/-\n",
    "\n",
    "Which will be sorted by the Shuffle and Sort:\n",
    "code : cat easytest.txt |python mapper.py |sort\n",
    "1       [2,3]/0.0/BLACK/-\n",
    "2       [1,3,4,5]/inf/WHITE/-\n",
    "2       EMPTY/1.0/GRAY/-1,\n",
    "3       [1,4,2]/inf/WHITE/-\n",
    "3       EMPTY/1.0/GRAY/-1,\n",
    "4       [2,3]/inf/WHITE/-\n",
    "5       [2]/inf/WHITE/-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the reducers, of course, receive all data for a given key - in this case it means that\n",
    "they receive the data for all \"copies\" of each node.\n",
    "\n",
    "the reducers job is to take all this data and construct a new node using the non-null\n",
    "list of edges, the minimum distance, the darkest color, using this logic the output\n",
    "from our first iteration will be :\n",
    "\n",
    "cat easytest.txt |python mapper.py| sort | python reducer.py\n",
    "\n",
    "1       [2, 3]/0.0/BLACK/-\n",
    "2       [1, 3, 4, 5]/1.0/GRAY/-1,\n",
    "3       [1, 4, 2]/1.0/GRAY/-1,\n",
    "4       [2, 3]/inf/WHITE/-\n",
    "5       [2]/inf/WHITE/-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-680d01f2f3f8>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-680d01f2f3f8>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    print '%s\\t%s' % (currentline, toprint)\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "#input comes from STDIN (standard input)\n",
    "currentline = \"\"\n",
    "currentdistance = float('inf')\n",
    "currentneighbours = \"EMPTY\"\n",
    "currentColor = \"WHITE\"\n",
    "currentPath = \"\"\n",
    "firstone = True\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    val = line.split(\"\\t\")\n",
    "    words = val[1].split(\"/\")\n",
    "    key = val[0]\n",
    "    if words[0] != 'EMPTY':\n",
    "        words[0] = eval(words[0]) \n",
    "    words[1] = float(words[1])\n",
    "    if key != currentline:\n",
    "        if not firstone:\n",
    "            toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "            print '%s\\t%s' % (currentline, toprint)\n",
    "            currentline = key\n",
    "            currentdistance = words[1]\n",
    "            currentneighbours = words[0]\n",
    "            currentColor = words[2]\n",
    "            currentPath = words[3]\n",
    "        else:\n",
    "            currentline = key\n",
    "            currentdistance = words[1]\n",
    "            currentneighbours = words[0]\n",
    "            currentColor = words[2]\n",
    "            currentPath = words[3]\n",
    "            firstone = False\n",
    "    if currentdistance > words[1]:\n",
    "        currentdistance = words[1]\n",
    "        currentPath = words[3]\n",
    "    if words[0] != 'EMPTY':\n",
    "        currentneighbours = words[0]\n",
    "    if words[2] == \"BLACK\":\n",
    "        currentColor = words[2]\n",
    "    if words[2] == \"GRAY\":\n",
    "        if currentColor == \"WHITE\":\n",
    "            currentColor = words[2]\n",
    "\n",
    "toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "print '%s\\t%s' % (currentline, toprint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The algorithm is iterative, we have to tail chain mapreduce job to have the final result.\n",
    "Here is the code used to perform the mapreduce job:\n",
    "\n",
    "#Assuming that we are connected to the cluster:\n",
    "cd Desktop/workspace/work/mr/\n",
    "wget https://www.dropbox.com/s/ec4hmdsvr50qxbe/preprofin.txt \n",
    "#Here we download the preprocessed input file to be treated\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/mr\n",
    "hdfs dfs -mkdir /user/hadoop/mr/input1\n",
    "hdfs dfs -put preprofin.txt /user/hadoop/mr/input1\n",
    "\n",
    "wget https://www.dropbox.com/s/64wlmdtq9ka8jes/mapper.py\n",
    "wget https://www.dropbox.com/s/6ppdmz7oukk3hj4/reducer.py\n",
    "chmod a+x reducer.py\n",
    "chmod a+x mapper.py\n",
    "\n",
    "perl -pe 's/\\r$//g' < mapper.py > mapperL.py\n",
    "perl -pe 's/\\r$//g' < reducer.py > reducerL.py\n",
    "#This step was used to transform windows file sustem to unix file system. without it, the mapreduce job was not succesfull.\n",
    "\n",
    "chmod a+x reducerL.py\n",
    "chmod a+x mapperL.py\n",
    "\n",
    "#Code below is used to chain multiple mapreduce jobs\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/input1 \\\n",
    "-output /user/hadoop/mr/output1 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/output1 \\\n",
    "-output /user/hadoop/mr/output2 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/output2 \\\n",
    "-output /user/hadoop/mr/output3 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapreduce job execution:\n",
    "    \n",
    "19/02/24 15:08:58 INFO mapreduce.Job: Job job_1551012986990_0017 completed successfully\n",
    "19/02/24 15:08:58 INFO mapreduce.Job: Counters: 51\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=7642\n",
    "                FILE: Number of bytes written=3033178\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=102096\n",
    "                HDFS: Number of bytes written=11361\n",
    "                HDFS: Number of read operations=69\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=14\n",
    "        Job Counters\n",
    "                Killed map tasks=1\n",
    "                Launched map tasks=16\n",
    "                Launched reduce tasks=7\n",
    "                Data-local map tasks=8\n",
    "                Rack-local map tasks=8\n",
    "                Total time spent by all maps in occupied slots (ms)=7778400\n",
    "                Total time spent by all reduces in occupied slots (ms)=3998304\n",
    "                Total time spent by all map tasks (ms)=162050\n",
    "                Total time spent by all reduce tasks (ms)=41649\n",
    "                Total vcore-milliseconds taken by all map tasks=162050\n",
    "                Total vcore-milliseconds taken by all reduce tasks=41649\n",
    "                Total megabyte-milliseconds taken by all map tasks=248908800\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=127945728\n",
    "        Map-Reduce Framework\n",
    "                Map input records=393\n",
    "                Map output records=403\n",
    "                Map output bytes=11574\n",
    "                Map output materialized bytes=12014\n",
    "                Input split bytes=2272\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=393\n",
    "                Reduce shuffle bytes=12014\n",
    "                Reduce input records=403\n",
    "                Reduce output records=393\n",
    "                Spilled Records=806\n",
    "                Shuffled Maps =112\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=112\n",
    "                GC time elapsed (ms)=4112\n",
    "                CPU time spent (ms)=19740\n",
    "                Physical memory (bytes) snapshot=7867559936\n",
    "                Virtual memory (bytes) snapshot=84767440896\n",
    "                Total committed heap usage (bytes)=7212630016\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=99824\n",
    "        File Output Format Counters\n",
    "                Bytes Written=11361\n",
    "            \n",
    "#SECOND RUN:\n",
    "\n",
    "19/03/01 21:57:40 INFO mapreduce.Job: Job job_1551476123970_0002 completed successfully\n",
    "19/03/01 21:57:40 INFO mapreduce.Job: Counters: 51\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=8076\n",
    "                FILE: Number of bytes written=3425652\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=21942\n",
    "                HDFS: Number of bytes written=11558\n",
    "                HDFS: Number of read operations=78\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=14\n",
    "        Job Counters\n",
    "                Killed map tasks=1\n",
    "                Launched map tasks=19\n",
    "                Launched reduce tasks=7\n",
    "                Data-local map tasks=16\n",
    "                Rack-local map tasks=3\n",
    "                Total time spent by all maps in occupied slots (ms)=8418768\n",
    "                Total time spent by all reduces in occupied slots (ms)=5070336\n",
    "                Total time spent by all map tasks (ms)=175391\n",
    "                Total time spent by all reduce tasks (ms)=52816\n",
    "                Total vcore-milliseconds taken by all map tasks=175391\n",
    "                Total vcore-milliseconds taken by all reduce tasks=52816\n",
    "                Total megabyte-milliseconds taken by all map tasks=269400576\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=162250752\n",
    "        Map-Reduce Framework\n",
    "                Map input records=393\n",
    "                Map output records=434\n",
    "                Map output bytes=12422\n",
    "                Map output materialized bytes=10828\n",
    "                Input split bytes=2660\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=395\n",
    "                Reduce shuffle bytes=10828\n",
    "                Reduce input records=434\n",
    "                Reduce output records=395\n",
    "                Spilled Records=868\n",
    "                Shuffled Maps =133\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=133\n",
    "                GC time elapsed (ms)=4402\n",
    "                CPU time spent (ms)=21330\n",
    "                Physical memory (bytes) snapshot=9021509632\n",
    "                Virtual memory (bytes) snapshot=94561951744\n",
    "                Total committed heap usage (bytes)=8306819072\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=19282\n",
    "        File Output Format Counters\n",
    "                Bytes Written=11558"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As this is an iterative job, we must define when the iterations should stop: \n",
    "Here it is interesting to stop the iterations when there is no GRAY line, meaning that all the possible paths from the initial node have been calculated.\n",
    "For this, we use a simple python script : graycount.py\n",
    "\n",
    "    At the end of each mapreduce job, we get the merged output file and test for it's number of gray lines:\n",
    "    hdfs dfs -getmerge /user/hadoop/mr/output5 output5.txt\n",
    "    python graycount.py output5.txt\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graycount.py\n",
    "\n",
    "import sys\n",
    "\n",
    "inp = sys.argv[1]\n",
    "r = open(inp, \"r\")\n",
    "lines = r.readlines()\n",
    "nb = 0\n",
    "for line in lines:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    val = line.split(\"\\t\")\n",
    "    words = val[1].split(\"/\")\n",
    "    \n",
    "    if words[2] == 'GRAY':\n",
    "        nb += 1\n",
    "print(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(inp, out):\n",
    "    r = open(inp, \"r\")\n",
    "    lines = r.readlines()\n",
    "    file = open(out,\"w\") \n",
    "    \n",
    "    for line in lines:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        val = line.split(\"\\t\")\n",
    "        words = val[1].split(\"/\")\n",
    "        key = val[0]\n",
    "        words[1] = float(words[1])\n",
    "        if words[2] == 'GRAY':\n",
    "            toprint = \"/\".join([words[0], str(words[1]), \"BLACK\", words[3]])\n",
    "            file.write(key + \"\\t\" + toprint + \"\\n\")\n",
    "            if words[0] != \"EMPTY\":\n",
    "                words[0] = eval(words[0])\n",
    "                for i in words[0]:\n",
    "                    toprint = \"/\".join([\"EMPTY\", str(words[1] + 1), \"GRAY\", str(words[3])+ str(key)+ \",\"])\n",
    "                    file.write(str(i) + \"\\t\" + toprint + \"\\n\")\n",
    "        else:\n",
    "            toprint = \"/\".join([words[0], str(words[1]), words[2], words[3]])\n",
    "            file.write(key + \"\\t\" + toprint + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(inp, out):\n",
    "    r = open(inp, \"r\")\n",
    "    lines = r.readlines()\n",
    "    file = open(out,\"w\") \n",
    "        \n",
    "    currentline = \"\"\n",
    "    currentdistance = float('inf')\n",
    "    currentneighbours = \"EMPTY\"\n",
    "    currentColor = \"WHITE\"\n",
    "    currentPath = \"\"\n",
    "    firstone = True\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        val = line.split(\"\\t\")\n",
    "        words = val[1].split(\"/\")\n",
    "        key = val[0]\n",
    "        if words[0] != 'EMPTY':\n",
    "            words[0] = eval(words[0]) \n",
    "        words[1] = float(words[1])\n",
    "        if key != currentline:\n",
    "            if not firstone:\n",
    "                toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "                file.write(currentline + \"\\t\" + toprint + \"\\n\")\n",
    "                currentline = key\n",
    "                currentdistance = words[1]\n",
    "                currentneighbours = words[0]\n",
    "                currentColor = words[2]\n",
    "                currentPath = words[3]\n",
    "            else:\n",
    "                currentline = key\n",
    "                currentdistance = words[1]\n",
    "                currentneighbours = words[0]\n",
    "                currentColor = words[2]\n",
    "                currentPath = words[3]\n",
    "                firstone = False\n",
    "        if currentdistance > words[1]:\n",
    "            currentdistance = words[1]\n",
    "            currentPath = words[3]\n",
    "        if words[0] != 'EMPTY':\n",
    "            currentneighbours = words[0]\n",
    "        if words[2] == \"BLACK\":\n",
    "            currentColor = words[2]\n",
    "        if words[2] == \"GRAY\":\n",
    "            if currentColor == \"WHITE\":\n",
    "                currentColor = words[2]\n",
    "\n",
    "    toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "    file.write(currentline + \"\\t\" + toprint + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graycount.py\n",
    "def graycount(inp):\n",
    "    r = open(inp, \"r\")\n",
    "    lines = r.readlines()\n",
    "    nb = 0\n",
    "    for line in lines:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        val = line.split(\"\\t\")\n",
    "        words = val[1].split(\"/\")\n",
    "\n",
    "        if words[2] == 'GRAY':\n",
    "            nb += 1\n",
    "    print(nb)\n",
    "    return(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shufflesort\n",
    "import pandas as pd\n",
    "def SS(inp, out):\n",
    "    df = pd.read_csv(inp, sep=\"\\t\", names=('from', 'info'))\n",
    "    df = df.sort_values(by=['from'])\n",
    "    df.to_csv(out, index=False, header= False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "36\n",
      "67\n",
      "77\n",
      "70\n",
      "31\n",
      "15\n",
      "4\n",
      "1\n",
      "0\n",
      "END\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "mapper(\"preprofin.txt\", \"map1.txt\")\n",
    "SS(\"map1.txt\", \"SS1.txt\")\n",
    "reducer(\"SS1.txt\", \"out1.txt\")\n",
    "graycount(\"out1.txt\")\n",
    "\n",
    "i = 1\n",
    "while graycount(\"out\" + str(i) + \".txt\") != 0:\n",
    "    mapper(\"out\" + str(i) + \".txt\", \"map\" + str(i +1) + \".txt\")\n",
    "    SS(\"map\" + str(i+1) + \".txt\", \"SS\" + str(i+1) + \".txt\")\n",
    "    reducer(\"SS\" + str(i+1) + \".txt\", \"out\" + str(i+1) + \".txt\")\n",
    "    i += 1\n",
    "print(\"END\")\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This shows us that we need 10 iterations in order to have all the paths processed.\n",
    "We can implement the same code in bash cmd code in order to implement an automatic mapreduce chaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dirshkla algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Comparison of our algorithm to Dijkstra\n",
    "Dikjstra algorithm is for nodes with different distances between each other. In this case, Dijkstra's algorithm is more efficient because at any step it only pursues edges from the minimum-cost path inside the frontier.\n",
    "The MapReduce version explores all paths in parallel; not as efficient overall, but the architecture is more scalable \n",
    "\n",
    "However, as in our case the distance is always = 1, our algorithm is equivalent to Dijkstra in efficiency.\n",
    "\n",
    "\n",
    "BLABLABLA\n",
    "https://courses.cs.washington.edu/courses/cse490h/08au/lectures/algorithms.pdf\n",
    "http://www.cs.ucc.ie/~gprovan/CS4407/2014/L16-Dijkstra-BFS.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
