{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecole polytechnique \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIG DATA FOR BUISNESS : DATABASE MANAGEMENT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Single source shortest path based on Djikstra Algorithm_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Souhail El Aissaoui - Hassan Lantry*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> _The task is to find shortest paths from a source node to all other nodes in the graph_<font>\n",
    "    \n",
    "<font color = black> This problem is solved by the Dijkstra’s algorithm, which is sequential.The project has a double purpose.<br>\n",
    "    \n",
    "First get familiar with Dijkstra’s algorithm, then devise a MapReduce version of the algorithm. <br>\n",
    "\n",
    "As you will realise, the process is actually iterative, so the\n",
    "identified MapReduce job must be iterated a certain number of times.\n",
    "We will do so by implementing in both Python Hadoop and Spark via a pyspark ecosystem \n",
    "\n",
    "Optional: perform scalability experiments as for previous projects. A single comparison on a reasonable big graph would be sufficient <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. *__Project goal__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The purpose of this algorithm is to find shortest path of a source node to all other nodes in a graph. This problem is solved by the Dijkstra’s algorithm. This project has double purposes. First, to get familiar with Dijkstra’s algorithm, then implement a MapReduce version of it. The algorithm is iterative, so the identified MapReduce job must be iterated several times to find the final solution. We provided a Python-Hadoop streaming and Spark (Python) implementations of the algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. *__Dijkstra Algorithm__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dji.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dijkstra's algorithm (or Dijkstra's Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.\n",
    "\n",
    "The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes,but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. *__Let's Implement it in Python Hadoop__*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We assume that a graph is a set of edge pairs (v,w), indicating that an edge exists from node v to node w.\n",
    "\n",
    "The initial non processed input file \"graph.txt\" is as following: we take into account that each line is of the form « v edge-label ww », so in order to represent a couple « v w » only the first and third elements should be retained.\n",
    "\n",
    "The analytics problem is on directed graphs, it consists of finding the best path from a specific source node that we will name the \"Initial Node\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>B</th>\n",
       "      <th>to</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>5247</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578</td>\n",
       "      <td>1672</td>\n",
       "      <td>502</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "      <td>6623</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>532</td>\n",
       "      <td>2373</td>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>1007</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from     B   to    D\n",
       "0     4  5247  274  274\n",
       "1   578  1672  502  502\n",
       "2    94  6623  359  359\n",
       "3   532  2373  800  800\n",
       "4    64  1007    5    5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "df = pd.read_csv('graph.txt', delim_whitespace=True, names=('from', 'B', 'to', 'D'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We preprocess this input file to be use din our MapReduce streaming.\n",
    "We will need it to be at the format:\n",
    "INPUT FORMAT: This data will be used for testing the program with linux pipe line.\n",
    "1\t[2,3]/0/GRAY/-\n",
    "2\t[1,3,4,5]/inf/WHITE/-\n",
    "3\t[1,4,2]/inf/WHITE/-\n",
    "4\t[2,3]/inf/WHITE/-\n",
    "5\t[2]/inf/WHITE/-\n",
    "\n",
    "Which stands for:\n",
    "FirstNode     [SET of paired nodes]/distanceFromBeginning/Color/BestPath"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first Node is the node from which the edges begins.\n",
    "The Set of paired nodes is a list of all the nodes where an edge from firsNode ends.\n",
    "The distance is the calculated distance from the initial node to the FirstNode. It will be initialised as infinite for all unknown pairs, and as 0 for the initial node.\n",
    "The color is used to know is the node has been visited or not by our algorithm. So it's starts off as white for all the nodes, exept for the initial node which will start as GRAY to be explored by the mapper. After exploration by the mapper, the nodes will take the color \"BLACK\".\n",
    "The BestPath will be written by the mapper and reducer which will keep all the nodes that have been browsen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.1 *__Data preprocessing__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>d</th>\n",
       "      <th>state</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[48, 2, 63, 136, 564, 73, 377, 141, 63, 4]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 164, 129, 19, 9, 321, 12, 536]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[274, 766, 74, 23, 762, 142, 782, 394]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[51, 199, 633, 524, 79, 394]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[79, 5, 134]</td>\n",
       "      <td>inf</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from                                          to    d  state path\n",
       "0     1  [48, 2, 63, 136, 564, 73, 377, 141, 63, 4]  0.0   GRAY    -\n",
       "1     2          [1, 164, 129, 19, 9, 321, 12, 536]  inf  WHITE    -\n",
       "2     4      [274, 766, 74, 23, 762, 142, 782, 394]  inf  WHITE    -\n",
       "3     5                [51, 199, 633, 524, 79, 394]  inf  WHITE    -\n",
       "4     8                                [79, 5, 134]  inf  WHITE    -"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing:\n",
    "df = df.drop(['B', 'D'], axis=1)\n",
    "df = df.groupby('from')['to'].apply(list)\n",
    "df2 = pd.DataFrame()\n",
    "df2['from'] = df.index\n",
    "df2['to'] = df.values\n",
    "df2['d'] = math.inf\n",
    "df2['state'] = 'WHITE'\n",
    "df2['path'] = '-'\n",
    "#Seeting the initial and final nodes : \n",
    "initial = 1\n",
    "final = 316\n",
    "df2.loc[df2['from']==initial, 'd'] = 0\n",
    "df2.loc[df2['from']==initial, 'state'] = 'GRAY'\n",
    "df2\n",
    "\n",
    "file = open(\"preprofin.txt\",\"w\") \n",
    "for index, row in df2.iterrows():\n",
    "    file.write(str(row[0]) + \"\\t\" + str(row[1]) + \"/\" + str(row[2]) + \"/\" + str(row[3]) + \"/\"+ str(row[4]) + \"\\n\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.2 *__Mapper.py__*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The mapper takes as input format:\n",
    "code : cat easytest.txt\n",
    "1\t[2,3]/0/GRAY/-\n",
    "2\t[1,3,4,5]/inf/WHITE/-\n",
    "3\t[1,4,2]/inf/WHITE/-\n",
    "4\t[2,3]/inf/WHITE/-\n",
    "5\t[2]/inf/WHITE/-\n",
    "\n",
    "The mappers are responsible for \"exploding\" all gray nodes - e.g. for exploding all nodes that live at our current depth in the tree, for each gray node, the mappers emit a new gray node, with distance = distance + 1. they also then emit the input gray node, but colored black. (once a node has been exploded, we're done with it.)\n",
    "\n",
    "Mappers also emit all non-gray nodes, with no change.\n",
    "\n",
    "Note that when the mappers \"explode\" the gray nodes and create a new node for each edge, they do not know what to write for the edges of this new node - so they leave it blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    val = line.split(\"\\t\")\n",
    "    words = val[1].split(\"/\")\n",
    "    key = val[0]\n",
    "    words[1] = float(words[1])\n",
    "    if words[2] == 'GRAY':\n",
    "        toprint = \"/\".join([words[0], str(words[1]), \"BLACK\", words[3]])\n",
    "        print '%s\\t%s' % (key, toprint)\n",
    "        if words[0] != \"EMPTY\":\n",
    "\t\t\twords[0] = eval(words[0])\n",
    "\t\t\tfor i in words[0]:\n",
    "\t\t\t\ttoprint = \"/\".join([\"EMPTY\", str(words[1] + 1), \"GRAY\", str(words[3])+ str(key)+ \",\"])\n",
    "\t\t\t\tprint '%s\\t%s' % (i, toprint)\n",
    "    else:\n",
    "        toprint = \"/\".join([words[0], str(words[1]), words[2], words[3]])\n",
    "        print '%s\\t%s' % (key, toprint)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The mapper output is:\n",
    "code : cat easytest.txt | python mapper.py\n",
    "\n",
    "1       [2,3]/0.0/BLACK/-\n",
    "2       EMPTY/1.0/GRAY/-1,\n",
    "3       EMPTY/1.0/GRAY/-1,\n",
    "2       [1,3,4,5]/inf/WHITE/-\n",
    "3       [1,4,2]/inf/WHITE/-\n",
    "4       [2,3]/inf/WHITE/-\n",
    "5       [2]/inf/WHITE/-\n",
    "\n",
    "Which will be sorted by the Shuffle and Sort:\n",
    "code : cat easytest.txt |python mapper.py |sort\n",
    "\n",
    "1       [2,3]/0.0/BLACK/-\n",
    "2       [1,3,4,5]/inf/WHITE/-\n",
    "2       EMPTY/1.0/GRAY/-1,\n",
    "3       [1,4,2]/inf/WHITE/-\n",
    "3       EMPTY/1.0/GRAY/-1,\n",
    "4       [2,3]/inf/WHITE/-\n",
    "5       [2]/inf/WHITE/-\n",
    "\n",
    "The output from the shuffle and sort must be \"merged\" by the reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.2 *__Reducer.py__*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The reducers, of course, receive all data for a given key - in this case it means that\n",
    "they receive the data for all \"copies\" of each node.\n",
    "\n",
    "The reducers job is to take all this data and construct a new node using the non-null\n",
    "list of edges, the minimum distance and the darkest color. Using this logic the output from our first iteration will be :\n",
    "\n",
    "code : cat easytest.txt |python mapper.py| sort | python reducer.py\n",
    "\n",
    "1       [2, 3]/0.0/BLACK/-\n",
    "2       [1, 3, 4, 5]/1.0/GRAY/-1,\n",
    "3       [1, 4, 2]/1.0/GRAY/-1,\n",
    "4       [2, 3]/inf/WHITE/-\n",
    "5       [2]/inf/WHITE/-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys\n",
    "#input comes from STDIN (standard input)\n",
    "currentline = \"\"\n",
    "currentdistance = float('inf')\n",
    "currentneighbours = \"EMPTY\"\n",
    "currentColor = \"WHITE\"\n",
    "currentPath = \"\"\n",
    "firstone = True\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    val = line.split(\"\\t\")\n",
    "    words = val[1].split(\"/\")\n",
    "    key = val[0]\n",
    "    if words[0] != 'EMPTY':\n",
    "        words[0] = eval(words[0]) \n",
    "    words[1] = float(words[1])\n",
    "    if key != currentline:\n",
    "        if not firstone:\n",
    "            toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "            print '%s\\t%s' % (currentline, toprint)\n",
    "            currentline = key\n",
    "            currentdistance = words[1]\n",
    "            currentneighbours = words[0]\n",
    "            currentColor = words[2]\n",
    "            currentPath = words[3]\n",
    "        else:\n",
    "            currentline = key\n",
    "            currentdistance = words[1]\n",
    "            currentneighbours = words[0]\n",
    "            currentColor = words[2]\n",
    "            currentPath = words[3]\n",
    "            firstone = False\n",
    "    if currentdistance > words[1]:\n",
    "        currentdistance = words[1]\n",
    "        currentPath = words[3]\n",
    "    if words[0] != 'EMPTY':\n",
    "        currentneighbours = words[0]\n",
    "    if words[2] == \"BLACK\":\n",
    "        currentColor = words[2]\n",
    "    if words[2] == \"GRAY\":\n",
    "        if currentColor == \"WHITE\":\n",
    "            currentColor = words[2]\n",
    "\n",
    "toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "print '%s\\t%s' % (currentline, toprint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.3 *Tail chain of the mapreduce job*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The algorithm is iterative, we have to tail chain mapreduce job to have the final result.\n",
    "Here is the code used to perform the mapreduce job:\n",
    "\n",
    "\n",
    "#Assuming that we are connected to the cluster:\n",
    "cd Desktop/workspace/work/mr/\n",
    "wget https://www.dropbox.com/s/ec4hmdsvr50qxbe/preprofin.txt \n",
    "#Here we download the preprocessed input file to be treated\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/mr\n",
    "hdfs dfs -mkdir /user/hadoop/mr/input1\n",
    "hdfs dfs -put preprofin.txt /user/hadoop/mr/input1\n",
    "\n",
    "wget https://www.dropbox.com/s/64wlmdtq9ka8jes/mapper.py\n",
    "wget https://www.dropbox.com/s/6ppdmz7oukk3hj4/reducer.py\n",
    "chmod a+x reducer.py\n",
    "chmod a+x mapper.py\n",
    "\n",
    "perl -pe 's/\\r$//g' < mapper.py > mapperL.py\n",
    "perl -pe 's/\\r$//g' < reducer.py > reducerL.py\n",
    "#This step was used to transform windows file sustem to unix file system. without it, the mapreduce job was not succesfull.\n",
    "\n",
    "chmod a+x reducerL.py\n",
    "chmod a+x mapperL.py\n",
    "\n",
    "#Code below is used to chain multiple three mapreduce jobs. \n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/input1 \\\n",
    "-output /user/hadoop/mr/output1 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/output1 \\\n",
    "-output /user/hadoop/mr/output2 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/output2 \\\n",
    "-output /user/hadoop/mr/output3 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As this is an iterative job, we must define when the iterations should stop: \n",
    "Here it is interesting to stop the iterations when there is no GRAY line, meaning that all the possible paths from the initial node have been calculated.\n",
    "For this, we use a simple python script : graycount.py\n",
    "\n",
    "    At the end of each mapreduce job, we get the merged output file and test for it's number of gray lines:\n",
    "    hdfs dfs -getmerge /user/hadoop/mr/output5 output5.txt\n",
    "    python graycount.py output5.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graycount.py\n",
    "\n",
    "import sys\n",
    "\n",
    "inp = sys.argv[1]\n",
    "r = open(inp, \"r\")\n",
    "lines = r.readlines()\n",
    "nb = 0\n",
    "for line in lines:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    val = line.split(\"\\t\")\n",
    "    words = val[1].split(\"/\")\n",
    "    \n",
    "    if words[2] == 'GRAY':\n",
    "        nb += 1\n",
    "print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapreduce job execution:\n",
    "    \n",
    "19/02/24 15:08:58 INFO mapreduce.Job: Job job_1551012986990_0017 completed successfully\n",
    "19/02/24 15:08:58 INFO mapreduce.Job: Counters: 51\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=7642\n",
    "                FILE: Number of bytes written=3033178\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=102096\n",
    "                HDFS: Number of bytes written=11361\n",
    "                HDFS: Number of read operations=69\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=14\n",
    "        Job Counters\n",
    "                Killed map tasks=1\n",
    "                Launched map tasks=16\n",
    "                Launched reduce tasks=7\n",
    "                Data-local map tasks=8\n",
    "                Rack-local map tasks=8\n",
    "                Total time spent by all maps in occupied slots (ms)=7778400\n",
    "                Total time spent by all reduces in occupied slots (ms)=3998304\n",
    "                Total time spent by all map tasks (ms)=162050\n",
    "                Total time spent by all reduce tasks (ms)=41649\n",
    "                Total vcore-milliseconds taken by all map tasks=162050\n",
    "                Total vcore-milliseconds taken by all reduce tasks=41649\n",
    "                Total megabyte-milliseconds taken by all map tasks=248908800\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=127945728\n",
    "        Map-Reduce Framework\n",
    "                Map input records=393\n",
    "                Map output records=403\n",
    "                Map output bytes=11574\n",
    "                Map output materialized bytes=12014\n",
    "                Input split bytes=2272\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=393\n",
    "                Reduce shuffle bytes=12014\n",
    "                Reduce input records=403\n",
    "                Reduce output records=393\n",
    "                Spilled Records=806\n",
    "                Shuffled Maps =112\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=112\n",
    "                GC time elapsed (ms)=4112\n",
    "                CPU time spent (ms)=19740\n",
    "                Physical memory (bytes) snapshot=7867559936\n",
    "                Virtual memory (bytes) snapshot=84767440896\n",
    "                Total committed heap usage (bytes)=7212630016\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=99824\n",
    "        File Output Format Counters\n",
    "                Bytes Written=11361\n",
    "            \n",
    "#SECOND RUN:\n",
    "\n",
    "19/03/01 21:57:40 INFO mapreduce.Job: Job job_1551476123970_0002 completed successfully\n",
    "19/03/01 21:57:40 INFO mapreduce.Job: Counters: 51\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=8076\n",
    "                FILE: Number of bytes written=3425652\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=21942\n",
    "                HDFS: Number of bytes written=11558\n",
    "                HDFS: Number of read operations=78\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=14\n",
    "        Job Counters\n",
    "                Killed map tasks=1\n",
    "                Launched map tasks=19\n",
    "                Launched reduce tasks=7\n",
    "                Data-local map tasks=16\n",
    "                Rack-local map tasks=3\n",
    "                Total time spent by all maps in occupied slots (ms)=8418768\n",
    "                Total time spent by all reduces in occupied slots (ms)=5070336\n",
    "                Total time spent by all map tasks (ms)=175391\n",
    "                Total time spent by all reduce tasks (ms)=52816\n",
    "                Total vcore-milliseconds taken by all map tasks=175391\n",
    "                Total vcore-milliseconds taken by all reduce tasks=52816\n",
    "                Total megabyte-milliseconds taken by all map tasks=269400576\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=162250752\n",
    "        Map-Reduce Framework\n",
    "                Map input records=393\n",
    "                Map output records=434\n",
    "                Map output bytes=12422\n",
    "                Map output materialized bytes=10828\n",
    "                Input split bytes=2660\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=395\n",
    "                Reduce shuffle bytes=10828\n",
    "                Reduce input records=434\n",
    "                Reduce output records=395\n",
    "                Spilled Records=868\n",
    "                Shuffled Maps =133\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=133\n",
    "                GC time elapsed (ms)=4402\n",
    "                CPU time spent (ms)=21330\n",
    "                Physical memory (bytes) snapshot=9021509632\n",
    "                Virtual memory (bytes) snapshot=94561951744\n",
    "                Total committed heap usage (bytes)=8306819072\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters\n",
    "                Bytes Read=19282\n",
    "        File Output Format Counters\n",
    "                Bytes Written=11558"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The tenths output file has zero \"GRAY\" lines. This means that all the paths from initial nodes have been calculated.\n",
    "This tenth's iteration's result have been stored in \"finalOUTPUT.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Local simulation of the job"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to simulate the mapreduce job, we can define the mapper et reducer functions, combined with a sorter, to calculate how many iterations are needed to obtain the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(inp, out):\n",
    "    r = open(inp, \"r\")\n",
    "    lines = r.readlines()\n",
    "    file = open(out,\"w\") \n",
    "    \n",
    "    for line in lines:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        val = line.split(\"\\t\")\n",
    "        words = val[1].split(\"/\")\n",
    "        key = val[0]\n",
    "        words[1] = float(words[1])\n",
    "        if words[2] == 'GRAY':\n",
    "            toprint = \"/\".join([words[0], str(words[1]), \"BLACK\", words[3]])\n",
    "            file.write(key + \"\\t\" + toprint + \"\\n\")\n",
    "            if words[0] != \"EMPTY\":\n",
    "                words[0] = eval(words[0])\n",
    "                for i in words[0]:\n",
    "                    toprint = \"/\".join([\"EMPTY\", str(words[1] + 1), \"GRAY\", str(words[3])+ str(key)+ \",\"])\n",
    "                    file.write(str(i) + \"\\t\" + toprint + \"\\n\")\n",
    "        else:\n",
    "            toprint = \"/\".join([words[0], str(words[1]), words[2], words[3]])\n",
    "            file.write(key + \"\\t\" + toprint + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(inp, out):\n",
    "    r = open(inp, \"r\")\n",
    "    lines = r.readlines()\n",
    "    file = open(out,\"w\") \n",
    "        \n",
    "    currentline = \"\"\n",
    "    currentdistance = float('inf')\n",
    "    currentneighbours = \"EMPTY\"\n",
    "    currentColor = \"WHITE\"\n",
    "    currentPath = \"\"\n",
    "    firstone = True\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        val = line.split(\"\\t\")\n",
    "        words = val[1].split(\"/\")\n",
    "        key = val[0]\n",
    "        if words[0] != 'EMPTY':\n",
    "            words[0] = eval(words[0]) \n",
    "        words[1] = float(words[1])\n",
    "        if key != currentline:\n",
    "            if not firstone:\n",
    "                toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "                file.write(currentline + \"\\t\" + toprint + \"\\n\")\n",
    "                currentline = key\n",
    "                currentdistance = words[1]\n",
    "                currentneighbours = words[0]\n",
    "                currentColor = words[2]\n",
    "                currentPath = words[3]\n",
    "            else:\n",
    "                currentline = key\n",
    "                currentdistance = words[1]\n",
    "                currentneighbours = words[0]\n",
    "                currentColor = words[2]\n",
    "                currentPath = words[3]\n",
    "                firstone = False\n",
    "        if currentdistance > words[1]:\n",
    "            currentdistance = words[1]\n",
    "            currentPath = words[3]\n",
    "        if words[0] != 'EMPTY':\n",
    "            currentneighbours = words[0]\n",
    "        if words[2] == \"BLACK\":\n",
    "            currentColor = words[2]\n",
    "        if words[2] == \"GRAY\":\n",
    "            if currentColor == \"WHITE\":\n",
    "                currentColor = words[2]\n",
    "\n",
    "    toprint = \"/\".join([str(currentneighbours), str(currentdistance), currentColor, currentPath])\n",
    "    file.write(currentline + \"\\t\" + toprint + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graycount.py\n",
    "def graycount(inp):\n",
    "    r = open(inp, \"r\")\n",
    "    lines = r.readlines()\n",
    "    nb = 0\n",
    "    for line in lines:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # split the line into words\n",
    "        val = line.split(\"\\t\")\n",
    "        words = val[1].split(\"/\")\n",
    "\n",
    "        if words[2] == 'GRAY':\n",
    "            nb += 1\n",
    "    print(nb)\n",
    "    return(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shufflesort\n",
    "import pandas as pd\n",
    "def SS(inp, out):\n",
    "    df = pd.read_csv(inp, sep=\"\\t\", names=('from', 'info'))\n",
    "    df = df.sort_values(by=['from'])\n",
    "    df.to_csv(out, index=False, header= False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that all the functions have been defined, we must chain the jobs until there is no gray line.\n",
    "Each iteration prints us the number of gray lines in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "36\n",
      "67\n",
      "77\n",
      "70\n",
      "31\n",
      "15\n",
      "4\n",
      "1\n",
      "0\n",
      "Number of jobs needed to obtain final results: \n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#first job:\n",
    "\n",
    "mapper(\"preprofin.txt\", \"map1.txt\")\n",
    "SS(\"map1.txt\", \"SS1.txt\")\n",
    "reducer(\"SS1.txt\", \"out1.txt\")\n",
    "graycount(\"out1.txt\")\n",
    "\n",
    "#iteration until no gray line lasts\n",
    "\n",
    "i = 1\n",
    "while graycount(\"out\" + str(i) + \".txt\") != 0:\n",
    "    mapper(\"out\" + str(i) + \".txt\", \"map\" + str(i +1) + \".txt\")\n",
    "    SS(\"map\" + str(i+1) + \".txt\", \"SS\" + str(i+1) + \".txt\")\n",
    "    reducer(\"SS\" + str(i+1) + \".txt\", \"out\" + str(i+1) + \".txt\")\n",
    "    i += 1\n",
    "    \n",
    "print(\"Number of jobs needed to obtain final results: \")\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This shows us that we need 10 iterations in order to have all the paths processed.\n",
    "We can implement the same code in bash cmd code in order to implement an automatic mapreduce chaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 MapReduce job Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to run the job on the cluster, these commands has been executed with 10 iterations.\n",
    "At each iteration, the output file was analysed by the python script graycount to be aware when the output is final.\n",
    "\n",
    "\n",
    "code ( 3 first iterations only) :\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/input1 \\\n",
    "-output /user/hadoop/mr/output1 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n",
    "hdfs dfs -getmerge /user/hadoop/mr/output1 output1.txt\n",
    "\n",
    "python graycount.py output1.txt\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/output1 \\\n",
    "-output /user/hadoop/mr/output2 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n",
    "hdfs dfs -getmerge /user/hadoop/mr/output2 output2.txt\n",
    "\n",
    "python graycount.py output2.txt\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/mr/output2 \\\n",
    "-output /user/hadoop/mr/output3 \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-mapper /home/hadoop/Desktop/workspace/work/mr/mapperL.py \\\n",
    "-file /home/hadoop/Desktop/workspace/work/mr/reducerL.py \\\n",
    "-reducer /home/hadoop/Desktop/workspace/work/mr/reducerL.py\n",
    "\n",
    "hdfs dfs -getmerge /user/hadoop/mr/output3 output3.txt\n",
    "\n",
    "python graycount.py output3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112     [547, 110]/inf/WHITE/-\n",
      "\n",
      "119     [19]/4.0/BLACK/-1,2,164,195,\n",
      "\n",
      "126     [445, 189, 196, 151, 251, 657, 133]/3.0/BLACK/-1,141,16,\n",
      "\n",
      "133     [141, 32, 379, 136, 8]/2.0/BLACK/-1,377,\n",
      "\n",
      "16      [54, 5, 66, 1, 516, 81, 35, 126, 269, 78, 766, 1, 547, 20]/2.0/BLACK/-1,141,\n",
      "\n",
      "161     [157, 158, 629]/5.0/BLACK/-1,136,26,286,348,\n",
      "\n",
      "168     [51]/7.0/BLACK/-1,73,449,567,219,86,226,\n",
      "\n",
      "175     [214, 422]/4.0/BLACK/-1,136,26,203,\n",
      "\n",
      "182     [619]/5.0/BLACK/-1,2,9,352,105,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = open(\"finalOUTPUT.txt\", \"r\")\n",
    "lines = r.readlines()\n",
    "for l in lines[1:10]:\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The final output: \"FINALoutput.txt\" contains all the shortest path from the initial node to all the connected nodes.\n",
    "Here, the first line indicates that for the node number 112, there is no path from the initial node.\n",
    "For the the node number 119, the shortest path pass through 4 nodes ( distance = 4), which are nodes 1,2,164 and 195.\n",
    "\n",
    "This output can be used to search the shortest paths to every node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. *__Let's Implement it in Spark__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.1 *__Spark Implementation__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing in Hadoop we create a script in Spark that follows a pattern following the same logic, a loop with a breaking condition when the distance at every node no longer change at the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"preprofin.txt\")\n",
    "\n",
    "count = sc.accumulator(0)\n",
    "\n",
    "def customSplitNodesTextFile(node):\n",
    "\tif len(node.split(' ')) < 3:\n",
    "\t\tnid, distance = node.split(' ')\n",
    "\t\tneighbors = None\n",
    "\telse:\n",
    "\t\tnid, distance, neighbors = node.split(' ')\n",
    "\t\tneighbors = neighbors.split(':')\n",
    "\t\tneighbors = neighbors[:len(neighbors) - 1]\n",
    "\tpath = nid\n",
    "\treturn (nid , (int(distance), neighbors, path))\n",
    "\n",
    "def customSplitNodesIterative(node):\n",
    "\tnid = node[0]\n",
    "\tdistance = node[1][0]\n",
    "\tneighbors = node[1][1]\n",
    "\tpath = node[1][2]\n",
    "\telements = path.split('->')\n",
    "\tif elements[len(elements) - 1] != nid:\n",
    "\t\tpath = path + '->' + nid;\n",
    "\treturn (nid , (int(distance), neighbors, path))\n",
    "\n",
    "def customSplitNeighbor(parentPath, parentDistance, neighbor):\n",
    "\tif neighbor!=None:\n",
    "\t\tnid, distance = neighbor.split(',')\n",
    "\t\tdistance = parentDistance + int(distance)\n",
    "\t\tpath = parentPath + '->' + nid\n",
    "\t\treturn (nid, (int(distance), 'None', path))\n",
    "\n",
    "def minDistance(nodeValue1, nodeValue2):\n",
    "\tneighbors = None\n",
    "\tdistance = 0\n",
    "\tpath = ''\n",
    "\tif nodeValue1[1] != 'None':\n",
    "\t\tneighbors = nodeValue1[1]\n",
    "\telse:\n",
    "\t\tneighbors = nodeValue2[1]\n",
    "\tdist1 = nodeValue1[0]\n",
    "\tdist2 = nodeValue2[0]\n",
    "\tif dist1 <= dist2:\n",
    "\t\tdistance = dist1\n",
    "\t\tpath = nodeValue1[2]\n",
    "\telse:\n",
    "\t\tcount.add(1);\n",
    "\t\tdistance = dist2\n",
    "\t\tpath = nodeValue2[2]\n",
    "\treturn (distance, neighbors, path)\n",
    "\n",
    "def formatResult(node):\n",
    "\tnid = node[0]\n",
    "\tminDistance = node[1][0]\n",
    "\tpath = node[1][2]\n",
    "\treturn nid, minDistance, path\n",
    "\n",
    "nodes = textFile.map(lambda node: customSplitNodesTextFile(node))\n",
    "\n",
    "oldCount = 0\n",
    "iterations = 0\n",
    "while True:\n",
    "\titerations += 1\n",
    "\tnodesValues = nodes.map(lambda x: x[1])\n",
    "\tneighbors = nodesValues.filter(lambda nodeDataFilter: nodeDataFilter[1]!=None).map(\n",
    "\t\tlambda nodeData: map(\n",
    "\t\t\tlambda neighbor: customSplitNeighbor(\n",
    "\t\t\t\tnodeData[2], nodeData[0], neighbor\n",
    "\t\t\t), nodeData[1]\n",
    "\t\t)\n",
    "\t).flatMap(lambda x: x)\n",
    "\tmapper = nodes.union(neighbors)\n",
    "\treducer = mapper.reduceByKey(lambda x, y: minDistance(x, y))\n",
    "\tnodes = reducer.map(lambda node: customSplitNodesIterative(node))\n",
    "\tnodes.count() # We call the count to execute all the RDD transformations\n",
    "\tif oldCount == count.value:\n",
    "\t\tbreak\n",
    "\toldCount=count.value\n",
    "\n",
    "print('Finished after: ' + str(iterations) + ' iterations')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
